"""
M√≥dulo: data_io
----------------

Fornece fun√ß√µes utilit√°rias padronizadas para leitura e escrita de dados em
diferentes formatos de dados (excel, csv, parquet, etc) utilizados no ecossistema do Verificador Inteligente de
Or√ßamentos de Obras.

Este m√≥dulo abstrai a complexidade de m√∫ltiplos formatos (CSV, Excel,
Parquet, JSON, Feather, Pickle), garantindo uma interface consistente para
todas as etapas do pipeline ‚Äî desde prototipa√ß√£o local at√© uso em produ√ß√£o.

üß© Funcionalidades principais:
------------------------------

1) read_data(file_path, sheet_name=None, header=0)
   - Detecta automaticamente o m√©todo de leitura a partir da extens√£o.
   - Suporta:
       .csv, .xlsx, .xls, .json, .parquet, .feather, .pkl
   - Permite leitura de abas espec√≠ficas em arquivos Excel.
   - Utilizado por:
       ‚Ä¢ Parsing de or√ßamentos
       ‚Ä¢ Testes unit√°rios
       ‚Ä¢ Pipelines determin√≠sticos

2) export_data(data, file_path, create_dirs=True)
   - Exporta DataFrames ou m√∫ltiplos DataFrames (multi-sheet Excel).
   - Cria diret√≥rios automaticamente, quando necess√°rio.
   - Suporta:
       .csv, .xlsx, .json, .parquet, .feather, .pkl
   - Utilizado por:
       ‚Ä¢ Gera√ß√£o de relat√≥rios t√©cnicos
       ‚Ä¢ Salvamento de artefatos do verificador
       ‚Ä¢ Outputs intermedi√°rios do pipeline

üéØ Motiva√ß√£o e valor:
---------------------
- Unifica a manipula√ß√£o de dados em todo o projeto.
- Reduz duplica√ß√£o de c√≥digo em parseadores, validadores e testes.
- Facilita a troca futura de formato sem alterar o restante do pipeline.
- Padroniza I/O para rodar em ambientes diversos (local, AWS, CI/CD).


üìÅ Localiza√ß√£o:
--------------
Faz parte da camada utilit√°ria `utils/`.

"""

__author__ = "Emerson V. Rafael (emervin)"
__copyright__ = "Verificador Inteligente de Or√ßamentos de Obras"
__credits__ = ["Emerson V. Rafael", "Lucas Ken", "Clarissa Simoyama"]
__license__ = "MIT"
__version__ = "1.0.0"
__maintainer__ = "Emerson V. Rafael (emervin), Lucas Ken (kushida), Clarissa Simoyama (simoyam)"
__squad__ = "DataCraft"
__email__ = "emersonssmile@gmail.com"
__status__ = "Development"

import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import json

import pandas as pd
from unidecode import unidecode

# Adiciona o diret√≥rio src ao path
base_dir = Path(__file__).parents[3]
sys.path.insert(0, str(Path(base_dir, "src")))

from config.config_logger import logger


def read_data(
    file_path: Union[str, Path],
    sheet_name: Optional[Union[str, int]] = None,
    header: Optional[Union[int, List[int]]] = 0,
    default_sheet: Optional[Union[str, List[str]]] = ["Sheet1", "Planilha1", "Plan1"],
) -> pd.DataFrame:
    """
    L√™ dados de v√°rios formatos de arquivo usando a extens√£o do arquivo para determinar o m√©todo apropriado.
    Se a aba especificada (sheet_name) n√£o existir, utiliza a aba padr√£o (default_sheet).

    Args:
        file_path (Union[str, Path]): Caminho para o arquivo a ser lido.
        sheet_name (Optional[Union[str, int]]): Nome ou √≠ndice da aba a ser lida (para arquivos Excel). Padr√£o √© None.
        header (Optional[Union[int, List[int]]]): N√∫mero(s) da(s) linha(s) a ser(em) usada(s) como nomes das colunas. Padr√£o √© 0.
        default_sheet (Optional[Union[str, List[str]]]): Nome ou lista de nomes das abas padr√£o a serem lidas se a aba especificada n√£o for encontrada.

    Returns:
        pd.DataFrame: DataFrame contendo os dados lidos.

    Raises:
        ValueError: Se a extens√£o do arquivo n√£o for suportada.
        FileNotFoundError: Se o arquivo n√£o existir.
    """
    file_path = Path(file_path)

    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")

    # Obtendo a extens√£o do dado recebido
    extension = file_path.suffix.lower()

    # Definindo os leitores dispon√≠veis no data functions
    readers = {
        ".csv": lambda path: pd.read_csv(path, header=header),
        ".xlsx": lambda path: pd.read_excel(path, sheet_name=sheet_name, header=header),
        ".xls": lambda path: pd.read_excel(path, sheet_name=sheet_name, header=header),
        ".xlsm": lambda path: pd.read_excel(
            path, sheet_name=sheet_name, header=header
        ),  # Added support for .xlsm files
        ".json": lambda path: pd.read_json(path),
        ".parquet": lambda path: pd.read_parquet(path),
        ".feather": lambda path: pd.read_feather(path),
        ".pkl": lambda path: pd.read_pickle(path),
    }

    reader = readers.get(extension)
    if reader is None:
        raise ValueError(f"Unsupported file extension: {extension}")

    try:
        # Tenta carregar a aba especificada
        return reader(file_path)
    except ValueError as e:
        # Tratamento espec√≠fico para erro de aba n√£o encontrada
        if "Worksheet named" in str(e) and "not found" in str(e):
            try:
                # Listar todas as abas dispon√≠veis no arquivo
                available_sheets = pd.ExcelFile(file_path).sheet_names
                if isinstance(default_sheet, str) and default_sheet in available_sheets:
                    logger.warning(
                        f"Aba '{sheet_name}' n√£o encontrada. Carregando a aba padr√£o '{default_sheet}'."
                    )
                    return pd.read_excel(file_path, sheet_name=default_sheet, header=header)
                elif isinstance(default_sheet, list):
                    for sheet in default_sheet:
                        if sheet in available_sheets:
                            logger.warning(
                                f"Aba '{sheet_name}' n√£o encontrada. Carregando a aba padr√£o '{sheet}'."
                            )
                            return pd.read_excel(file_path, sheet_name=sheet, header=header)
                raise ValueError(
                    f"Aba '{sheet_name}' n√£o encontrada no arquivo '{file_path}'. "
                    f"As abas dispon√≠veis s√£o: {available_sheets}"
                )
            except Exception as inner_e:
                raise RuntimeError(
                    f"Erro ao tentar listar as abas dispon√≠veis no arquivo '{file_path}': {str(inner_e)}"
                )
        else:
            raise e
    except Exception as e:
        logger.error(f"Erro ao ler o arquivo {file_path}: {str(e)}")


def export_data(
    data: Union[pd.DataFrame, Dict[str, pd.DataFrame]],
    file_path: Union[str, Path],
    create_dirs: bool = True,
    index: bool = False,
    **kwargs,
) -> None:
    """
    Exporta dados para v√°rios formatos, com suporte para m√∫ltiplas abas em arquivos Excel.

    Args:
        data (Union[pd.DataFrame, Dict[str, pd.DataFrame]]): DataFrame ou dicion√°rio de DataFrames para exporta√ß√£o.
        file_path (Union[str, Path]): Caminho onde o arquivo ser√° salvo.
        create_dirs (bool): Se True, cria diret√≥rios automaticamente se n√£o existirem. Default √© True.
        index (bool): Se True, inclui o √≠ndice ao salvar os dados. Default √© False.
        **kwargs: Argumentos adicionais passados para a fun√ß√£o de exporta√ß√£o do pandas.

    Raises:
        ValueError: Se a extens√£o do arquivo n√£o for suportada.
        RuntimeError: Se ocorrer um erro durante a exporta√ß√£o.
    """
    file_path = Path(file_path)

    if create_dirs:
        file_path.parent.mkdir(parents=True, exist_ok=True)

    extension = file_path.suffix.lower()

    exporters = {
        ".csv": lambda df, path: df.to_csv(path, index=index, **kwargs),
        ".xlsx": lambda df, path: (
            df.to_excel(path, index=index, **kwargs)
            if isinstance(df, pd.DataFrame)
            else (_export_multiple_sheets(df, path, index=index, **kwargs))
        ),
        ".json": lambda df, path: df.to_json(path, **kwargs),
        ".parquet": lambda df, path: df.to_parquet(path, **kwargs),
        ".feather": lambda df, path: df.to_feather(path, **kwargs),
        ".pkl": lambda df, path: df.to_pickle(path, **kwargs),
    }

    exporter = exporters.get(extension)
    if exporter is None:
        raise ValueError(f"Unsupported file extension: {extension}")

    try:
        exporter(data, file_path)
    except Exception as e:
        raise RuntimeError(f"Error exporting to {file_path}: {str(e)}")


def _export_multiple_sheets(
    data: Dict[str, pd.DataFrame], path: Union[str, Path], index: bool = False, **kwargs
):
    """
    Fun√ß√£o auxiliar para exportar m√∫ltiplas abas para um arquivo Excel.

    Args:
        data (Dict[str, pd.DataFrame]): Dicion√°rio de DataFrames para exporta√ß√£o.
        path (Union[str, Path]): Caminho para o arquivo Excel.
        index (bool): Se True, inclui o √≠ndice ao salvar os dados. Default √© False.
        **kwargs: Argumentos adicionais para pandas.to_excel.
    """
    with pd.ExcelWriter(path, engine="openpyxl") as writer:
        for sheet_name, sheet_data in data.items():
            sheet_data.to_excel(writer, sheet_name=sheet_name, index=index, **kwargs)


def transform_case(
    df: pd.DataFrame,
    columns_to_upper: Union[List[str], str, bool] = None,
    cells_to_upper: Union[List[str], str, bool] = None,
    columns_to_lower: Union[List[str], str, bool] = None,
    cells_to_lower: Union[List[str], str, bool] = None,
    columns_to_remove_spaces: Union[List[str], str, bool] = None,
    cells_to_remove_spaces: Union[List[str], str, bool] = None,
    columns_to_remove_accents: Union[List[str], str, bool] = None,
    cells_to_remove_accents: Union[List[str], str, bool] = None,
) -> pd.DataFrame:
    """
    Aplica transforma√ß√µes espec√≠ficas em colunas e c√©lulas de um DataFrame, como transformar em mai√∫sculas/min√∫sculas,
    remover espa√ßos e remover acentos.

    Args:
        df (pd.DataFrame): DataFrame a ser transformado.
        columns_to_upper (Union[List[str], str, bool]): Colunas para transformar os nomes em mai√∫sculas. Use True para todas.
        cells_to_upper (Union[List[str], str, bool]): Colunas para transformar os valores das c√©lulas em mai√∫sculas. Use True para todas.
        columns_to_lower (Union[List[str], str, bool]): Colunas para transformar os nomes em min√∫sculas. Use True para todas.
        cells_to_lower (Union[List[str], str, bool]): Colunas para transformar os valores das c√©lulas em min√∫sculas. Use True para todas.
        columns_to_remove_spaces (Union[List[str], str, bool]): Colunas para remover espa√ßos dos nomes. Use True para todas.
        cells_to_remove_spaces (Union[List[str], str, bool]): Colunas para remover espa√ßos dos valores das c√©lulas. Use True para todas.
        columns_to_remove_accents (Union[List[str], str, bool]): Colunas para remover acentos dos nomes. Use True para todas.
        cells_to_remove_accents (Union[List[str], str, bool]): Colunas para remover acentos dos valores das c√©lulas. Use True para todas.

    Returns:
        pd.DataFrame: DataFrame com as transforma√ß√µes aplicadas.
    """

    def transform_value(
        value, to_upper=False, to_lower=False, remove_spaces=False, remove_accents=False
    ):
        """Aplica transforma√ß√µes a um √∫nico valor."""
        if isinstance(value, str):
            if remove_accents:
                value = unidecode(value)
            if remove_spaces:
                value = value.replace(" ", "")
            if to_upper:
                value = value.upper()
            if to_lower:
                value = value.lower()
        return value

    def resolve_columns(param):
        """Resolve o par√¢metro para retornar uma lista de colunas."""
        if param in [True, "true", "True"]:
            return df.columns.tolist()
        elif isinstance(param, str):
            return [param]
        elif isinstance(param, list):
            return param
        return []

    # Garantir que os nomes das colunas sejam strings
    df.columns = df.columns.map(str)

    # Resolver colunas para cada transforma√ß√£o
    columns_to_upper = resolve_columns(columns_to_upper)
    cells_to_upper = resolve_columns(cells_to_upper)
    columns_to_lower = resolve_columns(columns_to_lower)
    cells_to_lower = resolve_columns(cells_to_lower)
    columns_to_remove_spaces = resolve_columns(columns_to_remove_spaces)
    cells_to_remove_spaces = resolve_columns(cells_to_remove_spaces)
    columns_to_remove_accents = resolve_columns(columns_to_remove_accents)
    cells_to_remove_accents = resolve_columns(cells_to_remove_accents)

    # Mapeamento para rastrear mudan√ßas nos nomes das colunas
    column_mapping = {col: col for col in df.columns}

    # Transformar nomes de colunas
    if columns_to_upper:
        column_mapping.update(
            {col: col.upper() for col in columns_to_upper if col in column_mapping}
        )
    if columns_to_lower:
        column_mapping.update(
            {col: col.lower() for col in columns_to_lower if col in column_mapping}
        )
    if columns_to_remove_spaces:
        column_mapping.update(
            {col: col.replace(" ", "") for col in columns_to_remove_spaces if col in column_mapping}
        )
    if columns_to_remove_accents:
        column_mapping.update(
            {col: unidecode(col) for col in columns_to_remove_accents if col in column_mapping}
        )

    # Atualizar os nomes das colunas no DataFrame
    df.rename(columns=column_mapping, inplace=True)

    # Atualizar os par√¢metros de c√©lulas com os novos nomes das colunas
    def update_column_list(columns):
        if columns:
            return [column_mapping.get(col, col) for col in columns]
        return []

    cells_to_upper = update_column_list(cells_to_upper)
    cells_to_lower = update_column_list(cells_to_lower)
    cells_to_remove_spaces = update_column_list(cells_to_remove_spaces)
    cells_to_remove_accents = update_column_list(cells_to_remove_accents)

    # Transformar valores das c√©lulas
    if cells_to_upper:
        for col in cells_to_upper:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: transform_value(x, to_upper=True))
    if cells_to_lower:
        for col in cells_to_lower:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: transform_value(x, to_lower=True))
    if cells_to_remove_spaces:
        for col in cells_to_remove_spaces:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: transform_value(x, remove_spaces=True))
    if cells_to_remove_accents:
        for col in cells_to_remove_accents:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: transform_value(x, remove_accents=True))

    return df


def filter_columns(df: pd.DataFrame, columns: list, allow_partial: bool = True) -> pd.DataFrame:
    """
    Filtra as colunas de um DataFrame com base em uma lista de colunas fornecida.

    Args:
        df (pd.DataFrame): DataFrame a ser filtrado.
        columns (list): Lista de colunas a serem mantidas no DataFrame.
        allow_partial (bool): Se True, mant√©m apenas as colunas existentes no DataFrame.
                             Se False, gera um erro se alguma coluna n√£o existir.

    Returns:
        pd.DataFrame: DataFrame filtrado com as colunas especificadas.

    Raises:
        ValueError: Se `allow_partial` for False e alguma coluna n√£o existir no DataFrame.
    """
    # Verifica as colunas que existem no DataFrame
    existing_columns = [col for col in columns if col in df.columns]

    # Se n√£o permitir parcial e houver colunas faltantes, gera um erro
    if not allow_partial and len(existing_columns) != len(columns):
        missing_columns = [col for col in columns if col not in df.columns]
        raise ValueError(f"As seguintes colunas est√£o ausentes no DataFrame: {missing_columns}")

    # Retorna o DataFrame filtrado com as colunas existentes
    return df[existing_columns]


def rename_columns(df: pd.DataFrame, rename_dict: Union[dict, "Box"]) -> pd.DataFrame:
    """
    Renomeia as colunas de um DataFrame de forma resiliente, lidando com colunas NaN e colunas inexistentes.

    Args:
        df (pd.DataFrame): DataFrame cujas colunas ser√£o renomeadas.
        rename_dict (Union[dict, Box]): Dicion√°rio ou Box (Dynaconf) contendo o mapeamento de renomea√ß√£o.

    Returns:
        pd.DataFrame: DataFrame com as colunas renomeadas.
    """
    # Converte Box para dict, se necess√°rio
    if not isinstance(rename_dict, dict):
        rename_dict = dict(rename_dict)

    # Substitui colunas NaN por strings vazias
    df.columns = df.columns.fillna("")

    # Filtra o rename_dict para incluir apenas colunas que existem no DataFrame
    valid_rename_dict = {col: rename_dict[col] for col in rename_dict if col in df.columns}

    # Renomeia as colunas do DataFrame
    df = df.rename(columns=valid_rename_dict)

    return df


def ensure_columns_exist(df: pd.DataFrame, columns: list, default_value=None) -> pd.DataFrame:
    """
    Garante que todas as colunas especificadas existam no DataFrame, criando-as com um valor padr√£o se necess√°rio.

    Args:
        df (pd.DataFrame): DataFrame a ser verificado.
        columns (list): Lista de colunas que devem existir no DataFrame.
        default_value (Any): Valor padr√£o para preencher as colunas criadas. Default √© None.

    Returns:
        pd.DataFrame: DataFrame com as colunas garantidas.
    """
    for col in columns:
        if col not in df.columns:
            df[col] = default_value  # Cria a coluna com o valor padr√£o
    return df


def select_columns(df: pd.DataFrame, target_columns: list) -> pd.DataFrame:
    """
    Seleciona colunas de um DataFrame com base em uma lista de colunas alvo, mantendo a ordem fornecida.

    Args:
        df (pd.DataFrame): DataFrame original.
        target_columns (list): Lista de nomes de colunas desejadas.

    Returns:
        pd.DataFrame: DataFrame com as colunas correspondentes selecionadas.
    """
    # Verifica quais colunas da lista alvo existem no DataFrame
    existing_columns = [col for col in target_columns if col in df.columns]

    # Retorna o DataFrame com as colunas existentes na ordem fornecida
    return df[existing_columns]


def export_to_json(
    data: Union[pd.DataFrame, Dict[str, Union[pd.DataFrame, dict]]],
    file_path: Union[str, Path],
    create_dirs: bool = True,
    orient: str = "records",
    **kwargs,
) -> None:
    """
    Exporta dados para o formato JSON, com suporte para criar diret√≥rios automaticamente.

    Args:
        data (Union[pd.DataFrame, Dict[str, Union[pd.DataFrame, dict]]]): DataFrame, dicion√°rio de DataFrames ou dicion√°rio de dados para exporta√ß√£o.
        file_path (Union[str, Path]): Caminho onde o arquivo JSON ser√° salvo.
        create_dirs (bool): Se True, cria diret√≥rios automaticamente se n√£o existirem. Default √© True.
        orient (str): Orienta√ß√£o do JSON (ex.: "records", "split", "index", etc.). Default √© "records".
        **kwargs: Argumentos adicionais passados para `DataFrame.to_json`.

    Raises:
        ValueError: Se o tipo de dado n√£o for suportado.
        RuntimeError: Se ocorrer um erro durante a exporta√ß√£o.
    """

    def default_serializer(obj):
        """Serializador padr√£o para objetos n√£o serializ√°veis pelo JSON."""
        if hasattr(obj, "__dict__"):
            return obj.__dict__
        return str(obj)

    file_path = Path(file_path)

    if create_dirs:
        file_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        if isinstance(data, pd.DataFrame):
            # Exporta um √∫nico DataFrame para JSON
            data.to_json(file_path, orient=orient, **kwargs)
        elif isinstance(data, dict):
            # Verifica se os valores do dicion√°rio s√£o DataFrames ou outros dicion√°rios
            json_data = {
                key: (df.to_dict(orient=orient) if isinstance(df, pd.DataFrame) else df)
                for key, df in data.items()
            }
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=4, default=default_serializer)
        else:
            raise ValueError(
                "O tipo de dado fornecido n√£o √© suportado. Use um DataFrame ou um dicion√°rio de DataFrames/dados."
            )
    except Exception as e:
        raise RuntimeError(f"Erro ao exportar para JSON em {file_path}: {str(e)}")


def cast_columns(df: pd.DataFrame, column_types: Dict[str, Union[str, type]]) -> pd.DataFrame:
    """
    Tenta converter as colunas de um DataFrame para os tipos especificados.

    Args:
        df (pd.DataFrame): O DataFrame a ser convertido.
        column_types (Dict[str, Union[str, type]]): Um dicion√°rio onde as chaves s√£o os nomes das colunas
                                                    e os valores s√£o os tipos esperados (ex.: "float64", int).

    Returns:
        pd.DataFrame: O DataFrame com as colunas convertidas.

    Raises:
        ValueError: Se uma coluna especificada n√£o existir no DataFrame ou se o tipo for inv√°lido.
    """
    valid_types = {
        "int",
        "float",
        "object",
        "category",
        "bool",
        "datetime64",
    }  # Tipos v√°lidos baseados no pandas

    for column, col_type in column_types.items():
        if column in df.columns:
            try:
                # Se col_type for um tipo num√©rico (ex.: float), aplica diretamente
                if isinstance(col_type, type):
                    df[column] = df[column].astype(col_type)
                else:
                    # Garante que col_type seja uma string
                    col_type = str(col_type).lower()
                    # Verifica se o tipo √© v√°lido
                    if not any(col_type.startswith(t) for t in valid_types):
                        logger.error(
                            f"Tipo de dado '{col_type}' n√£o √© v√°lido para a coluna '{column}'."
                        )
                    if col_type.startswith("int"):
                        # Preenche NaN com 0 antes de converter para int
                        df[column] = (
                            pd.to_numeric(df[column], errors="coerce").fillna(0).astype(col_type)
                        )

                        # Ap√≥s a convers√£o, substitui valores NaN por string vazia
                        df[column] = df[column].replace(
                            {0: ""} if col_type == "int" else {pd.NA: "", None: ""}
                        )
                    else:
                        df[column] = df[column].astype(col_type)
            except Exception as e:
                raise ValueError(
                    f"Erro ao converter a coluna '{column}' para o tipo '{col_type}': {e}"
                )
        else:
            raise ValueError(f"A coluna '{column}' n√£o existe no DataFrame.")
    return df


def merge_data(
    df_left: pd.DataFrame,
    df_right: pd.DataFrame,
    left_on: list,
    right_on: list,
    how: str = "inner",
    suffixes: tuple = ("_left", "_right"),
) -> pd.DataFrame:
    """
    Perform a generic merge between two DataFrames.

    Args:
        df_left (pd.DataFrame): The left DataFrame.
        df_right (pd.DataFrame): The right DataFrame.
        left_on (list): Columns to merge on from the left DataFrame.
        right_on (list): Columns to merge on from the right DataFrame.
        how (str): Type of merge to perform. Defaults to "inner".
        suffixes (tuple): Suffixes to apply to overlapping column names. Defaults to ("_left", "_right").

    Returns:
        pd.DataFrame: The merged DataFrame.

    Raises:
        ValueError: If the merge operation fails.
    """
    try:
        merged_df = pd.merge(
            df_left,
            df_right,
            left_on=left_on,
            right_on=right_on,
            how=how,
            suffixes=suffixes,
        )
        return merged_df
    except Exception as e:
        logger.error(f"Error during merge: {e}")
        raise ValueError(f"Error during merge: {e}")


def perform_merge(
    df_left: pd.DataFrame,
    df_right: pd.DataFrame,
    left_on: list,
    right_on: list,
    how: str,
    suffixes: tuple,
    validate: Optional[str],
    indicator: bool,
    handle_duplicates: bool,
) -> pd.DataFrame:
    """
    Fun√ß√£o auxiliar para realizar o merge entre dois DataFrames, com tratamento de duplicidades.

    Args:
        df_left (pd.DataFrame): DataFrame da esquerda.
        df_right (pd.DataFrame): DataFrame da direita.
        left_on (list): Colunas do DataFrame da esquerda para realizar o merge.
        right_on (list): Colunas do DataFrame da direita para realizar o merge.
        how (str): Tipo de merge a ser realizado.
        suffixes (tuple): Sufixos aplicados a colunas sobrepostas.
        validate (Optional[str]): Valida o tipo de relacionamento entre os DataFrames.
        indicator (bool): Se True, adiciona uma coluna indicando a origem de cada linha no merge.
        handle_duplicates (bool): Se True, remove duplicidades automaticamente em caso de erro.

    Returns:
        pd.DataFrame: DataFrame resultante do merge.
    """
    try:
        return pd.merge(
            df_left,
            df_right,
            left_on=left_on,
            right_on=right_on,
            how=how,
            suffixes=suffixes,
            validate=validate,
            indicator=indicator,
        )
    except pd.errors.MergeError as e:
        if handle_duplicates and "not a many-to-one merge" in str(e):
            logger.warning("Removendo duplicidades para tentar novamente o merge.")
            df_right = df_right.drop_duplicates(subset=right_on)
            return pd.merge(
                df_left,
                df_right,
                left_on=left_on,
                right_on=right_on,
                how=how,
                suffixes=suffixes,
                validate=validate,
                indicator=indicator,
            )
        logger.error(f"Erro durante o merge: {e}")
        raise ValueError(f"Erro durante o merge: {e}")


def merge_data_with_columns(
    df_left: pd.DataFrame,
    df_right: pd.DataFrame,
    left_on: list,
    right_on: list,
    selected_left_columns: list = None,
    selected_right_columns: list = None,
    how: str = "inner",
    suffixes: tuple = ("_left", "_right"),
    validate: str = None,
    indicator: bool = False,
    handle_duplicates: bool = False,
) -> pd.DataFrame:
    """
    Realiza um merge entre dois DataFrames, permitindo selecionar colunas espec√≠ficas e lidar com duplicidades.

    Par√¢metros:
        (mesmos par√¢metros da fun√ß√£o original)
    """
    # Filtra as colunas do DataFrame da esquerda, se especificado
    if selected_left_columns:
        df_left = df_left[left_on + selected_left_columns]

    # Filtra as colunas do DataFrame da direita, se especificado
    if selected_right_columns:
        df_right = df_right[right_on + selected_right_columns]

    # Realiza o merge usando a fun√ß√£o auxiliar
    return perform_merge(
        df_left=df_left,
        df_right=df_right,
        left_on=left_on,
        right_on=right_on,
        how=how,
        suffixes=suffixes,
        validate=validate,
        indicator=indicator,
        handle_duplicates=handle_duplicates,
    )


def two_stage_merge(
    left: pd.DataFrame,
    right: pd.DataFrame,
    keys_stage1: List[List[str]],  # colunas do LEFT (Budget)
    keys_stage2: List[List[str]],  # colunas do RIGHT (LPU)
    how: str = "left",
    suffixes: Tuple[str, str] = ("_l", "_r"),
    keep_indicator: bool = True,
    validate_stage1: Optional[str] = None,
    validate_stage2: Optional[str] = None,  # mantido por compatibilidade
    handle_duplicates: bool = False,
) -> pd.DataFrame:
    """
    Merge priorizado (sem duplicar linhas do LEFT), pareando regras por √≠ndice:

        regra i:
            left_on  = keys_stage1[i]
            right_on = keys_stage2[i]

    Exemplo (Budget x LPU):
        ['ID','REGIAO','GRUPO'] <-> ['C√ìD ITEM','REGIAO','GRUPO']
        ['NOME','REGIAO','GRUPO'] <-> ['ITEM','REGIAO','GRUPO']

    Comportamento:
    - tenta regra 1 no conjunto "remaining"
    - o que casar sai do remaining e entra no output
    - tenta regra 2 s√≥ no restante
    - sobras entram como left_only

    Retorna:
    - cada linha do left aparece no m√°ximo 1 vez
    - adiciona:
        _merge_stage: stage1 | none
        _merge_rule : rule_01, rule_02...
    """

    if how != "left":
        raise ValueError("two_stage_merge (consumo) suporta apenas how='left'.")

    def _to_list_str(x) -> List[str]:
        # BoxList/list/tuple -> list[str]
        return [str(c) for c in list(x)]

    # Normaliza BoxList -> list[str]
    left_keysets = [_to_list_str(k) for k in keys_stage1]
    right_keysets = [_to_list_str(k) for k in keys_stage2]

    # Garantia de pareamento por regra
    if len(left_keysets) != len(right_keysets):
        raise ValueError(
            f"keys_stage1 e keys_stage2 precisam ter o mesmo tamanho. "
            f"Recebido: {len(left_keysets)} vs {len(right_keysets)}."
        )

    # Preserva ordem original do left
    base_left = left.copy()
    row_id_col = "_row_id__tsm"
    if row_id_col in base_left.columns:
        raise ValueError(f"Coluna reservada j√° existe no DataFrame: {row_id_col}")

    # Adiciona coluna de ID (Int) tempor√°ria
    base_left[row_id_col] = range(len(base_left))

    # Mapeando todas as colunas que o dataframe possui
    base_left_cols = list(left.columns) + [row_id_col]

    # Inicia um dataframe com todos os dados
    remaining = base_left[base_left_cols].copy()

    # Inicia o dataframe que conter√° o resultado final (merged + unmerged)
    collected_parts: List[pd.DataFrame] = []

    def _clean_remaining(df: pd.DataFrame) -> pd.DataFrame:
        # Mant√©m apenas colunas do left (evita levar colunas do right para a pr√≥xima tentativa)
        return df[base_left_cols].copy()

    # Executa regras em ordem (prioridade)
    for i, (lkeys, rkeys) in enumerate(zip(left_keysets, right_keysets), start=1):

        # Verificando se h√° ainda linhas para processar
        if remaining.empty:
            break

        # Criando identificador da regra
        rule = f"rule_{i:02d}"
        logger.info(f"[two_stage_merge] Tentativa {i} | LEFT={lkeys} <-> RIGHT={rkeys}")

        # Executando o merge
        merged = perform_merge(
            df_left=remaining,
            df_right=right,
            left_on=lkeys,
            right_on=rkeys,
            how="left",
            suffixes=suffixes,
            validate=validate_stage1,
            indicator=True,
            handle_duplicates=handle_duplicates,
        )

        # Separa matched e unmatched
        matched = merged[merged["_merge"] == "both"].copy()
        unmatched = merged[merged["_merge"] == "left_only"].copy()

        if not matched.empty:
            matched["_merge_stage"] = f"stage{i}"
            matched["_merge_rule"] = rule
            collected_parts.append(matched)

        # mant√©m s√≥ o left para pr√≥xima tentativa
        unmatched = unmatched.drop(columns=["_merge"], errors="ignore")
        remaining = _clean_remaining(unmatched)

    # Adiciona as linhas restantes (n√£o casaram em nenhuma regra) ao resultado final.
    if not remaining.empty:
        remaining_out = remaining.copy()
        remaining_out["_merge"] = "left_only"
        remaining_out["_merge_stage"] = "none"
        remaining_out["_merge_rule"] = None
        collected_parts.append(remaining_out)

    # Consolida todas as partes (matched e unmatched) em um √∫nico DataFrame.
    out = pd.concat(collected_parts, ignore_index=True, sort=False)

    # Restaura a ordem original do DataFrame com base na coluna de ID tempor√°ria.
    out = out.sort_values(by=row_id_col, kind="stable").reset_index(drop=True)

    # Remove a coluna de ID tempor√°ria.
    out = out.drop(columns=[row_id_col], errors="ignore")

    # Remove a coluna "_merge" se o indicador n√£o for necess√°rio.
    if not keep_indicator:
        out = out.drop(columns=["_merge"], errors="ignore")

    # Retorna o DataFrame final consolidado
    return out


def concat_dataframes(
    dataframes: list, ignore_index: bool = True, fill_missing: bool = True
) -> pd.DataFrame:
    """
    Concatena uma lista de DataFrames, lidando com √≠ndices duplicados e colunas inconsistentes.

    Args:
        dataframes (list): Lista de DataFrames a serem concatenados.
        ignore_index (bool): Se deve ignorar os √≠ndices originais e criar um novo √≠ndice.
        fill_missing (bool): Se deve preencher valores ausentes com NaN para colunas inconsistentes.

    Returns:
        pd.DataFrame: DataFrame concatenado.
    """
    if not dataframes:
        raise ValueError("A lista de DataFrames est√° vazia.")

    # Verifica se todos os elementos da lista s√£o DataFrames
    if not all(isinstance(df, pd.DataFrame) for df in dataframes):
        raise TypeError("Todos os elementos da lista devem ser DataFrames.")

    # Preenche valores ausentes para colunas inconsistentes, se necess√°rio
    if fill_missing:
        all_columns = set(col for df in dataframes for col in df.columns)
        dataframes = [df.reindex(columns=all_columns) for df in dataframes]

    # Concatena os DataFrames
    concatenated_df = pd.concat(dataframes, ignore_index=ignore_index)

    return concatenated_df


def resolve_duplicate_columns(
    df: pd.DataFrame,
    column_name: Optional[str] = None,  # Nome da coluna ou None para todas as colunas
    strategy: str = "rename",  # Op√ß√µes: "rename", "keep_first", "keep_last", "drop"
    suffix: str = "_dup",
) -> pd.DataFrame:
    """
    Resolve colunas duplicadas em um DataFrame com base na estrat√©gia escolhida.

    Args:
        df (pd.DataFrame): DataFrame com poss√≠veis colunas duplicadas.
        column_name (Optional[str]): Nome da coluna a ser verificada para duplicatas.
                                      Use None para aplicar a estrat√©gia a todas as colunas.
        strategy (str): Estrat√©gia para lidar com duplicatas:
            - "rename": Renomeia colunas duplicadas adicionando um sufixo.
            - "keep_first": Mant√©m apenas a primeira ocorr√™ncia da coluna.
            - "keep_last": Mant√©m apenas a √∫ltima ocorr√™ncia da coluna.
            - "drop": Remove todas as colunas duplicadas.
        suffix (str): Sufixo a ser adicionado ao renomear colunas duplicadas (apenas para "rename").

    Returns:
        pd.DataFrame: DataFrame com as colunas duplicadas resolvidas.

    Raises:
        ValueError: Se a estrat√©gia fornecida n√£o for v√°lida.
    """

    # Fun√ß√£o auxiliar para renomear colunas duplicadas
    def rename_duplicates(columns):
        seen = {}
        new_columns = []
        for col in columns:
            if col in seen:
                seen[col] += 1
                new_columns.append(f"{col}{suffix}{seen[col]}")
            else:
                seen[col] = 0
                new_columns.append(col)
        return new_columns

    # logger.info(f"Colunas antes: {df.columns.tolist()}")

    # Se column_name for None, aplica a estrat√©gia a todas as colunas
    if column_name is None:
        if strategy == "rename":
            # Renomeia todas as colunas duplicadas no DataFrame
            df.columns = rename_duplicates(df.columns)
        elif strategy in ["keep_first", "keep_last", "drop"]:
            # Identifica colunas duplicadas
            duplicated_mask = df.columns.duplicated(keep=strategy.split("_")[1])
            if strategy == "drop":
                # Remove todas as duplicatas
                df = df.loc[:, ~df.columns.duplicated(keep=False)]
            else:
                # Mant√©m apenas a primeira ou √∫ltima ocorr√™ncia
                df = df.loc[:, ~duplicated_mask]
        else:
            raise ValueError(
                f"Estrat√©gia inv√°lida: '{strategy}'. Use 'rename', 'keep_first', 'keep_last' ou 'drop'."
            )
    else:
        # Aplica a estrat√©gia apenas √† coluna especificada
        if column_name not in df.columns:
            raise ValueError(f"A coluna '{column_name}' n√£o existe no DataFrame.")

        # Identifica colunas duplicadas com o mesmo nome
        duplicate_columns = [col for col in df.columns if col == column_name]

        # Se n√£o houver duplicatas, retorna o DataFrame original
        if len(duplicate_columns) <= 1:
            return df

        if strategy == "rename":
            # Renomeia colunas duplicadas adicionando um sufixo
            for i, col in enumerate(duplicate_columns[1:], start=1):
                new_name = f"{col}{suffix}{i}"
                df.rename(columns={col: new_name}, inplace=True)
        elif strategy == "keep_first":
            # Mant√©m apenas a primeira ocorr√™ncia da coluna
            df = df.loc[:, ~df.columns.duplicated(keep="first")]
        elif strategy == "keep_last":
            # Mant√©m apenas a √∫ltima ocorr√™ncia da coluna
            df = df.loc[:, ~df.columns.duplicated(keep="last")]
        elif strategy == "drop":
            # Remove todas as colunas duplicadas
            df = df.loc[:, ~df.columns.duplicated(keep=False)]
        else:
            raise ValueError(
                f"Estrat√©gia inv√°lida: '{strategy}'. Use 'rename', 'keep_first', 'keep_last' ou 'drop'."
            )

    # logger.info(f"Colunas depois: {df.columns.tolist()}")

    return df
